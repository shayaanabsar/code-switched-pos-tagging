{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from preprocessing import *\n",
    "from trainer import *\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessor()\n",
    "pp.read_data('dataset')\n",
    "input_tensor, output_tensor = pp.create_tensors()\n",
    "\n",
    "b_s, b_e = pp.splitters['bengali.csv']\n",
    "h_s, h_e = pp.splitters['hindi.csv'  ]\n",
    "t_s, t_e = pp.splitters['telugu.csv' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input, b_output = input_tensor[b_s:b_e], output_tensor[b_s:b_e]\n",
    "h_input, h_output = input_tensor[h_s:h_e], output_tensor[h_s:h_e]\n",
    "t_input, t_output = input_tensor[t_s:t_e], output_tensor[t_s:t_e]\n",
    "\n",
    "b, h, t, o  =  (b_e-b_s), (h_e-h_s), (t_e-t_s), input_tensor.shape[0]\n",
    "\n",
    "b_input_train, b_input_test, b_input_val    = b_input[:int(0.8*b)], b_input[int(0.8*b):int(0.9*b)], b_input[int(0.9*b):]\n",
    "h_input_train, h_input_test, h_input_val    = b_input[:int(0.8*h)], b_input[int(0.8*h):int(0.9*h)], h_input[int(0.9*h):]\n",
    "t_input_train, t_input_test, t_input_val    = t_input[:int(0.8*t)], t_input[int(0.8*b):int(0.9*t)], t_input[int(0.9*t):]\n",
    "\n",
    "b_output_train, b_output_test, b_output_val = b_output[:int(0.8*b)], b_output[int(0.8*b):int(0.9*b)], b_output[int(0.9*b):]\n",
    "h_output_train, h_output_test, h_output_val = b_output[:int(0.8*h)], b_output[int(0.8*h):int(0.9*h)], h_output[int(0.9*h):]\n",
    "t_output_train, t_output_test, t_output_val = t_output[:int(0.8*t)], t_output[int(0.8*b):int(0.9*t)], t_output[int(0.9*t):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_csi, b_si = torch.tensor(pp.cs_index[b_s:b_e]), torch.tensor(pp.s_index[b_s:b_e])\n",
    "h_csi, h_si = torch.tensor(pp.cs_index[h_s:h_e]), torch.tensor(pp.s_index[h_s:h_e])\n",
    "t_csi, t_si = torch.tensor(pp.cs_index[t_s:t_e]), torch.tensor(pp.s_index[t_s:t_e])\n",
    "o_csi, o_si = torch.tensor(pp.cs_index)         , torch.tensor(pp.s_index)\n",
    "\n",
    "b_csi_m, b_si_m = torch.mean(b_csi).item(), torch.mean(b_si).item()\n",
    "h_csi_m, h_si_m = torch.mean(h_csi).item(), torch.mean(h_si).item()\n",
    "t_csi_m, t_si_m = torch.mean(t_csi).item(), torch.mean(t_si).item()\n",
    "o_csi_m, o_si_m = torch.mean(o_csi).item(), torch.mean(o_si).item()\n",
    "\n",
    "coef = torch.corrcoef(torch.stack([o_csi, o_si]))[0][1]\n",
    "data = pd.DataFrame([\n",
    "\t['Hindi'  , h_csi_m, h_si_m, h], \n",
    "\t['Bengali', b_csi_m, b_si_m, b], \n",
    "\t['Telugu' , t_csi_m, t_si_m, t], \n",
    "\t['Overall', o_csi_m, o_si_m, o]\n",
    "])\n",
    "\n",
    "data.columns = ['Language', 'Mean CM-Index', 'Mean S-Index', 'Count']\n",
    "print(data, '\\n\\n')\n",
    "print(f'PMCC between the CMI (Gamb√§ck and Das) and the S-index: {coef:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Hyperparameters\n",
    "\n",
    "xlm_roberta             = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')\n",
    "xlm_roberta_output_size = 250002\n",
    "cross_entropy_loss      = nn.CrossEntropyLoss()\n",
    "num_tags                = b_output_train.shape[2]\n",
    "batch_size              = 16\n",
    "dropout_rate            = 0.2\n",
    "sequence_length         = pp.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.xlm_roberta    = xlm_roberta\n",
    "\t\tself.dropout        = nn.Dropout(dropout_rate)\n",
    "\t\tself.linear         = nn.Linear(xlm_roberta_output_size, num_tags)\n",
    "\t\tself.batch_norm     = nn.BatchNorm1d(num_features=sequence_length)\n",
    "\t\tself.softmax        = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\troberta_logits      = self.xlm_roberta(input).logits\n",
    "\t\tdropout_logits      = self.dropout(roberta_logits)\n",
    "\t\tmodel_logits        = self.linear(dropout_logits)\n",
    "\t\tnormalised_logits   = self.batch_norm(model_logits)\n",
    "\t\tmodel_probabilities = self.softmax(normalised_logits) \n",
    "\n",
    "\t\treturn model_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(Model(), cross_entropy_loss, 0.01)\n",
    "trainer.train(1, batch_size, b_input_test, b_output, b_input_val, b_output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
