{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from preprocessing import *\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessor()\n",
    "pp.read_data('dataset')\n",
    "input_tensor, output_tensor = pp.create_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s, b_e = pp.splitters['bengali.csv']\n",
    "h_s, h_e = pp.splitters['hindi.csv'  ]\n",
    "t_s, t_e = pp.splitters['telugu.csv' ]\n",
    "\n",
    "b_input, b_output = input_tensor[b_s:b_e], output_tensor[b_s:b_e]\n",
    "h_input, h_output = input_tensor[h_s:h_e], output_tensor[h_s:h_e]\n",
    "t_input, t_output = input_tensor[t_s:t_e], output_tensor[t_s:t_e]\n",
    "\n",
    "b, h, t  =  (b_e-b_s), (h_e-h_s), (t_e-t_s)\n",
    "\n",
    "b_input_train, b_input_test, b_input_val    = b_input[:int(0.8*b)], b_input[int(0.8*b):int(0.9*b)], b_input[int(0.9*b):]\n",
    "h_input_train, h_input_test, h_input_val    = b_input[:int(0.8*h)], b_input[int(0.8*h):int(0.9*h)], h_input[int(0.9*h):]\n",
    "t_input_train, t_input_test, t_input_val    = t_input[:int(0.8*t)], t_input[int(0.8*b):int(0.9*t)], t_input[int(0.9*t):]\n",
    "\n",
    "b_output_train, b_output_test, b_output_val = b_output[:int(0.8*b)], b_output[int(0.8*b):int(0.9*b)], b_output[int(0.9*b):]\n",
    "h_output_train, h_output_test, h_output_val = b_output[:int(0.8*h)], b_output[int(0.8*h):int(0.9*h)], h_output[int(0.9*h):]\n",
    "t_output_train, t_output_test, t_output_val = t_output[:int(0.8*t)], t_output[int(0.8*b):int(0.9*t)], t_output[int(0.9*t):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Hyperparameters\n",
    "\n",
    "xlm_roberta             = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')\n",
    "xlm_roberta_output_size = 250002\n",
    "num_tags                = b_output_train.shape[2]\n",
    "batch_size              = 16\n",
    "dropout_rate            = 0.2\n",
    "sequence_length         = pp.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.xlm_roberta = xlm_roberta\n",
    "\t\tself.dropout     = nn.Dropout(dropout_rate)\n",
    "\t\tself.linear      = nn.Linear(xlm_roberta_output_size, num_tags)\n",
    "\t\tself.batch_norm  = nn.BatchNorm1d(num_features=sequence_length)\n",
    "\t\tself.softmax     = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\troberta_logits      = self.xlm_roberta(input).logits\n",
    "\t\tdropout_logits      = self.dropout(roberta_logits)\n",
    "\t\tmodel_logits        = self.linear(dropout_logits)\n",
    "\t\tnormalised_logits   = self.batch_norm(model_logits)\n",
    "\t\tmodel_probabilities = self.softmax(normalised_logits) \n",
    "\n",
    "\t\treturn model_probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
